{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ask the user for input\n",
    "search_query = input(\"Enter the product you want to search for on Amazon: \")\n",
    "\n",
    "# Create the Amazon search URL\n",
    "url = f\"https://www.amazon.in/s?k={search_query}\"\n",
    "\n",
    "# Send an HTTP GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Process the search results here (you can display them or move on to Task 2)\n",
    "    # You might want to extract product details and links from the search results\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ad4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape product details from a single page\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    product_details = []\n",
    "\n",
    "    # Find all product elements on the page\n",
    "    product_elements = soup.find_all('div', class_='s-result-item')\n",
    "\n",
    "    for product in product_elements:\n",
    "        # Extract product details\n",
    "        title_element = product.find('span', class_='a-text-normal')\n",
    "        price_element = product.find('span', class_='a-offscreen')\n",
    "        return_exchange_element = product.find('span', class_='a-declarative')\n",
    "        expected_delivery_element = product.find('span', class_='a-text-bold')\n",
    "        availability_element = product.find('span', class_='a-size-small a-color-success')\n",
    "\n",
    "        # Check if all details are present, if not, replace with \"-\"\n",
    "        title = title_element.text.strip() if title_element else \"-\"\n",
    "        price = price_element.text.strip() if price_element else \"-\"\n",
    "        return_exchange = return_exchange_element.text.strip() if return_exchange_element else \"-\"\n",
    "        expected_delivery = expected_delivery_element.text.strip() if expected_delivery_element else \"-\"\n",
    "        availability = availability_element.text.strip() if availability_element else \"-\"\n",
    "\n",
    "        # Extract product URL\n",
    "        product_url = product.find('a', class_='a-link-normal')['href']\n",
    "        product_url = 'https://www.amazon.in' + product_url\n",
    "\n",
    "        product_details.append({\n",
    "            'Product Name': title,\n",
    "            'Price': price,\n",
    "            'Return/Exchange': return_exchange,\n",
    "            'Expected Delivery': expected_delivery,\n",
    "            'Availability': availability,\n",
    "            'Product URL': product_url\n",
    "        })\n",
    "\n",
    "    return product_details\n",
    "\n",
    "# Function to scrape product details from multiple pages\n",
    "def scrape_multiple_pages(search_query, max_pages):\n",
    "    all_product_details = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"https://www.amazon.in/s?k={search_query}&page={page}\"\n",
    "        product_details = scrape_page(url)\n",
    "\n",
    "        if product_details:\n",
    "            all_product_details.extend(product_details)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return all_product_details\n",
    "\n",
    "# User input for the product to search\n",
    "search_query = input(\"Enter the product you want to search for on Amazon: \")\n",
    "\n",
    "# Set the maximum number of pages to scrape (3 in this case)\n",
    "max_pages = 3\n",
    "\n",
    "# Scrape product details from multiple pages\n",
    "all_products = scrape_multiple_pages(search_query, max_pages)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f'{search_query}_products.csv', index=False)\n",
    "\n",
    "print(f\"Scraped {len(all_products)} products and saved to {search_query}_products.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288530bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Function to scrape images for a given keyword\n",
    "def scrape_images_for_keyword(driver, keyword, num_images=10):\n",
    "    # Navigate to Google Images\n",
    "    driver.get(\"https://www.google.com/imghp\")\n",
    "\n",
    "    # Locate the search bar element and enter the keyword\n",
    "    search_box = driver.find_element_by_name(\"q\")\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Scroll down to load more images (repeat several times)\n",
    "    for _ in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    # Find and extract image URLs\n",
    "    image_urls = []\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        src = img.get(\"src\")\n",
    "        if src and src.startswith(\"http\"):\n",
    "            image_urls.append(src)\n",
    "    \n",
    "    # Create a directory to save images\n",
    "    os.makedirs(keyword, exist_ok=True)\n",
    "\n",
    "    # Download and save images\n",
    "    for i, img_url in enumerate(image_urls[:num_images]):\n",
    "        response = requests.get(img_url)\n",
    "        if response.status_code == 200:\n",
    "            with open(f\"{keyword}/{keyword}_{i+1}.jpg\", \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "# Keywords to search for\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# Initialize the web driver (make sure you have installed the appropriate driver)\n",
    "# Example for Chrome:\n",
    "# driver = webdriver.Chrome(\"/path/to/chromedriver\")\n",
    "# Replace \"/path/to/chromedriver\" with the actual path to the Chrome WebDriver\n",
    "# Download Chrome WebDriver from https://sites.google.com/chromium.org/driver/\n",
    "# Make sure to put the WebDriver in a directory included in your PATH environment variable\n",
    "\n",
    "# Initialize Firefox driver\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# Loop through keywords and scrape images\n",
    "for keyword in keywords:\n",
    "    scrape_images_for_keyword(driver, keyword, num_images=10)\n",
    "\n",
    "# Close the web driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape smartphone details\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    url = f\"https://www.flipkart.com/search?q={search_query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User-Agent String\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        products = soup.find_all(\"div\", {\"class\": \"_1AtVbE\"})\n",
    "\n",
    "        smartphone_details = []\n",
    "\n",
    "        for product in products:\n",
    "            details = product.find(\"div\", {\"class\": \"_2kHMtA\"})\n",
    "            name = details.find(\"a\", {\"class\": \"IRpwTa\"}).text.strip()\n",
    "            url = \"https://www.flipkart.com\" + details.find(\"a\", {\"class\": \"IRpwTa\"})[\"href\"]\n",
    "\n",
    "            specifications = product.find(\"ul\", {\"class\": \"vFw0gD\"})\n",
    "            specifications = specifications.find_all(\"li\")\n",
    "\n",
    "            brand = name.split(\" \")[0]\n",
    "            color = specifications[0].text.strip() if len(specifications) > 0 else \"-\"\n",
    "            ram = specifications[1].text.strip() if len(specifications) > 1 else \"-\"\n",
    "            storage = specifications[2].text.strip() if len(specifications) > 2 else \"-\"\n",
    "            primary_camera = specifications[3].text.strip() if len(specifications) > 3 else \"-\"\n",
    "            secondary_camera = specifications[4].text.strip() if len(specifications) > 4 else \"-\"\n",
    "            display_size = specifications[5].text.strip() if len(specifications) > 5 else \"-\"\n",
    "            battery_capacity = specifications[6].text.strip() if len(specifications) > 6 else \"-\"\n",
    "            price = product.find(\"div\", {\"class\": \"_30jeq3\"}).text.strip()\n",
    "\n",
    "            smartphone_details.append({\n",
    "                \"Brand Name\": brand,\n",
    "                \"Smartphone Name\": name,\n",
    "                \"Colour\": color,\n",
    "                \"RAM\": ram,\n",
    "                \"Storage(ROM)\": storage,\n",
    "                \"Primary Camera\": primary_camera,\n",
    "                \"Secondary Camera\": secondary_camera,\n",
    "                \"Display Size\": display_size,\n",
    "                \"Battery Capacity\": battery_capacity,\n",
    "                \"Price\": price,\n",
    "                \"Product URL\": url\n",
    "            })\n",
    "\n",
    "        return smartphone_details\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        return []\n",
    "\n",
    "# Search query for smartphones\n",
    "search_query = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "\n",
    "# Scrape smartphone details\n",
    "smartphone_details = scrape_flipkart_smartphones(search_query)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(smartphone_details)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f'{search_query}_smartphones.csv', index=False)\n",
    "\n",
    "print(f\"Scraped {len(smartphone_details)} smartphones and saved to {search_query}_smartphones.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd1af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape geospatial coordinates of a city from Google Maps\n",
    "def scrape_coordinates(city_name):\n",
    "    # Create the Google Maps search URL\n",
    "    search_url = f\"https://www.google.com/maps/search/{city_name}\"\n",
    "\n",
    "    # Send an HTTP GET request\n",
    "    response = requests.get(search_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the element containing the coordinates\n",
    "        coordinates_element = soup.find('meta', itemprop='geo')\n",
    "\n",
    "        if coordinates_element:\n",
    "            # Extract latitude and longitude from the content attribute\n",
    "            content = coordinates_element['content']\n",
    "            latitude, longitude = content.split(',')\n",
    "\n",
    "            return {\n",
    "                'City': city_name,\n",
    "                'Latitude': latitude,\n",
    "                'Longitude': longitude\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Coordinates not found for {city_name}.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        return None\n",
    "\n",
    "# Input: City name to search for\n",
    "city_name = input(\"Enter the name of the city to search for on Google Maps: \")\n",
    "\n",
    "# Scrape coordinates\n",
    "coordinates = scrape_coordinates(city_name)\n",
    "\n",
    "# Display the result\n",
    "if coordinates:\n",
    "    print(f\"Coordinates for {city_name}:\")\n",
    "    print(f\"Latitude: {coordinates['Latitude']}\")\n",
    "    print(f\"Longitude: {coordinates['Longitude']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape details of best gaming laptops from digit.in\n",
    "def scrape_gaming_laptops():\n",
    "    # URL of the page with gaming laptop listings\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "    # Send an HTTP GET request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the container containing the laptop details\n",
    "        laptop_container = soup.find('div', class_='TopNumbeHeading')\n",
    "\n",
    "        # Initialize a list to store laptop details\n",
    "        laptop_details = []\n",
    "\n",
    "        if laptop_container:\n",
    "            # Find all the laptops listed\n",
    "            laptops = laptop_container.find_all('div', class_='right-container')\n",
    "\n",
    "            for laptop in laptops:\n",
    "                # Extract details for each laptop\n",
    "                laptop_name = laptop.find('h3').text.strip()\n",
    "                laptop_specs = laptop.find_all('li', class_='tVe95H')\n",
    "                \n",
    "                # Initialize variables to store specs\n",
    "                processor = \"-\"\n",
    "                ram = \"-\"\n",
    "                storage = \"-\"\n",
    "                display = \"-\"\n",
    "                gpu = \"-\"\n",
    "                price = \"-\"\n",
    "\n",
    "                for spec in laptop_specs:\n",
    "                    spec_text = spec.text.strip()\n",
    "                    if \"Processor\" in spec_text:\n",
    "                        processor = spec_text.split(\":\")[1].strip()\n",
    "                    elif \"RAM\" in spec_text:\n",
    "                        ram = spec_text.split(\":\")[1].strip()\n",
    "                    elif \"Storage\" in spec_text:\n",
    "                        storage = spec_text.split(\":\")[1].strip()\n",
    "                    elif \"Display\" in spec_text:\n",
    "                        display = spec_text.split(\":\")[1].strip()\n",
    "                    elif \"Graphics\" in spec_text:\n",
    "                        gpu = spec_text.split(\":\")[1].strip()\n",
    "                    elif \"Price\" in spec_text:\n",
    "                        price = spec_text.split(\":\")[1].strip()\n",
    "\n",
    "                laptop_details.append({\n",
    "                    'Laptop Name': laptop_name,\n",
    "                    'Processor': processor,\n",
    "                    'RAM': ram,\n",
    "                    'Storage': storage,\n",
    "                    'Display': display,\n",
    "                    'GPU': gpu,\n",
    "                    'Price': price\n",
    "                })\n",
    "\n",
    "        return laptop_details\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        return []\n",
    "\n",
    "# Scrape gaming laptop details\n",
    "gaming_laptops = scrape_gaming_laptops()\n",
    "\n",
    "# Display the scraped data (you can save it to a file or a DataFrame)\n",
    "for laptop in gaming_laptops:\n",
    "    print(\"Laptop Name:\", laptop['Laptop Name'])\n",
    "    print(\"Processor:\", laptop['Processor'])\n",
    "    print(\"RAM:\", laptop['RAM'])\n",
    "    print(\"Storage:\", laptop['Storage'])\n",
    "    print(\"Display:\", laptop['Display'])\n",
    "    print(\"GPU:\", laptop['GPU'])\n",
    "    print(\"Price:\", laptop['Price'])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape details of billionaires from Forbes\n",
    "def scrape_forbes_billionaires():\n",
    "    # URL of Forbes Billionaires page\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "    # Send an HTTP GET request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the container containing billionaire details\n",
    "        billionaires_container = soup.find('div', class_='table-fixed-body')\n",
    "\n",
    "        # Initialize a list to store billionaire details\n",
    "        billionaire_details = []\n",
    "\n",
    "        if billionaires_container:\n",
    "            # Find all the billionaire rows\n",
    "            billionaire_rows = billionaires_container.find_all('div', class_='table-row')\n",
    "\n",
    "            for row in billionaire_rows:\n",
    "                # Extract details for each billionaire\n",
    "                rank = row.find('div', class_='rank').text.strip()\n",
    "                name = row.find('div', class_='name').text.strip()\n",
    "                net_worth = row.find('div', class_='netWorth').text.strip()\n",
    "                age = row.find('div', class_='age').text.strip()\n",
    "                citizenship = row.find('div', class_='citizenship').text.strip()\n",
    "                source = row.find('div', class_='source').text.strip()\n",
    "                industry = row.find('div', class_='industry').text.strip()\n",
    "\n",
    "                billionaire_details.append({\n",
    "                    'Rank': rank,\n",
    "                    'Name': name,\n",
    "                    'Net Worth': net_worth,\n",
    "                    'Age': age,\n",
    "                    'Citizenship': citizenship,\n",
    "                    'Source': source,\n",
    "                    'Industry': industry\n",
    "                })\n",
    "\n",
    "        return billionaire_details\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        return []\n",
    "\n",
    "# Scrape billionaire details\n",
    "billionaires = scrape_forbes_billionaires()\n",
    "\n",
    "# Display the scraped data (you can save it to a file or a DataFrame)\n",
    "for billionaire in billionaires:\n",
    "    print(\"Rank:\", billionaire['Rank'])\n",
    "    print(\"Name:\", billionaire['Name'])\n",
    "    print(\"Net Worth:\", billionaire['Net Worth'])\n",
    "    print(\"Age:\", billionaire['Age'])\n",
    "    print(\"Citizenship:\", billionaire['Citizenship'])\n",
    "    print(\"Source:\", billionaire['Source'])\n",
    "    print(\"Industry:\", billionaire['Industry'])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc97154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Replace with your API Key\n",
    "api_key = 'AIzaSyDVoe-WSUi3dveEy_BGODr7jwQS4sME2uw'\n",
    "\n",
    "# Create a YouTube Data API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Specify the video ID of the YouTube video you want to retrieve comments for\n",
    "video_id = 'AIzaSyDVoe-WSUi3dveEy_BGODr7jwQS4sME2uw'\n",
    "\n",
    "# Maximum number of comments to retrieve\n",
    "max_results = 500\n",
    "\n",
    "# Retrieve comments for the video\n",
    "comments = []\n",
    "\n",
    "next_page_token = None\n",
    "\n",
    "while len(comments) < max_results:\n",
    "    results = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        maxResults=min(100, max_results - len(comments)),  # Limit to 100 comments per request\n",
    "        pageToken=next_page_token\n",
    "    ).execute()\n",
    "\n",
    "    for item in results['items']:\n",
    "        comment = item['snippet']['topLevelComment']['snippet']\n",
    "        comments.append({\n",
    "            'Comment': comment['textDisplay'],\n",
    "            'Upvotes': comment.get('likeCount', 0),\n",
    "            'Timestamp': comment['publishedAt']\n",
    "        })\n",
    "\n",
    "    if 'nextPageToken' in results:\n",
    "        next_page_token = results['nextPageToken']\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Display or save the extracted comments, upvotes, and timestamps\n",
    "for comment in comments:\n",
    "    print(f\"Comment: {comment['Comment']}\")\n",
    "    print(f\"Upvotes: {comment['Upvotes']}\")\n",
    "    print(f\"Timestamp: {comment['Timestamp']}\")\n",
    "    print('-' * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
