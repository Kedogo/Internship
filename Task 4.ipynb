{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ee9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_wikipedia_headers(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all header tags (h1 to h6) and extract their text\n",
    "        headers = [header.text for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
    "\n",
    "        # Create a DataFrame from the header data\n",
    "        headers_df = pd.DataFrame({'Headers': headers})\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return headers_df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "wikipedia_url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "headers_dataframe = scrape_wikipedia_headers(wikipedia_url)\n",
    "print(headers_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_former_presidents_india(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the table containing presidents' data\n",
    "        presidents_table = soup.find('table', {'class': 'tablepress'})\n",
    "\n",
    "        # Initialize empty lists to store data\n",
    "        names = []\n",
    "        terms = []\n",
    "\n",
    "        # Loop through table rows and extract data\n",
    "        for row in presidents_table.find_all('tr')[1:]:\n",
    "            columns = row.find_all('td')\n",
    "            name = columns[0].get_text(strip=True)\n",
    "            term = columns[1].get_text(strip=True)\n",
    "            names.append(name)\n",
    "            terms.append(term)\n",
    "\n",
    "        # Create a DataFrame from the extracted data\n",
    "        presidents_df = pd.DataFrame({'Name': names, 'Term of Office': terms})\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return presidents_df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "presidents_url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "presidents_dataframe = scrape_former_presidents_india(presidents_url)\n",
    "print(presidents_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_mens_odi_teams(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the table containing ODI team rankings\n",
    "        table = soup.find('table', {'class': 'table rankings-table'})\n",
    "\n",
    "        # Initialize lists to store data\n",
    "        teams = []\n",
    "        matches = []\n",
    "        points = []\n",
    "        ratings = []\n",
    "\n",
    "        # Loop through the table rows and extract data\n",
    "        for row in table.find_all('tr')[1:11]:  # Extract top 10 teams\n",
    "            columns = row.find_all('td')\n",
    "            team = columns[1].text.strip()\n",
    "            match = columns[2].text.strip()\n",
    "            point = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "\n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "\n",
    "        # Create a DataFrame from the extracted data\n",
    "        data = {\n",
    "            'Team': teams,\n",
    "            'Matches': matches,\n",
    "            'Points': points,\n",
    "            'Rating': ratings\n",
    "        }\n",
    "        teams_df = pd.DataFrame(data)\n",
    "\n",
    "        return teams_df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "icc_mens_url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "mens_odi_teams_df = scrape_mens_odi_teams(icc_mens_url)\n",
    "print(mens_odi_teams_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_womens_odi_teams(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the table containing ODI team rankings for women\n",
    "        table = soup.find('table', {'class': 'table rankings-table'})\n",
    "\n",
    "        # Initialize lists to store data\n",
    "        teams = []\n",
    "        matches = []\n",
    "        points = []\n",
    "        ratings = []\n",
    "\n",
    "        # Loop through the table rows and extract data\n",
    "        for row in table.find_all('tr')[1:11]:  # Extract top 10 teams\n",
    "            columns = row.find_all('td')\n",
    "            team = columns[1].text.strip()\n",
    "            match = columns[2].text.strip()\n",
    "            point = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "\n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "\n",
    "        # Create a DataFrame from the extracted data\n",
    "        data = {\n",
    "            'Team': teams,\n",
    "            'Matches': matches,\n",
    "            'Points': points,\n",
    "            'Rating': ratings\n",
    "        }\n",
    "        teams_df = pd.DataFrame(data)\n",
    "\n",
    "        return teams_df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "icc_womens_url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "womens_odi_teams_df = scrape_womens_odi_teams(icc_womens_url)\n",
    "print(womens_odi_teams_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68dcfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81236da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_news(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the container that holds the news articles\n",
    "        news_container = soup.find('div', {'class': 'Card'})\n",
    "        \n",
    "        # Initialize lists to store data\n",
    "        headlines = []\n",
    "        times = []\n",
    "        news_links = []\n",
    "\n",
    "        # Loop through the news articles and extract data\n",
    "        articles = news_container.find_all('div', {'class': 'Card-title'})\n",
    "        for article in articles:\n",
    "            headline = article.text.strip()\n",
    "            headlines.append(headline)\n",
    "            \n",
    "            # Find the time (you may need to adjust this based on the actual website structure)\n",
    "            time = article.find_next('div', {'class': 'Card-time'}).text.strip()\n",
    "            times.append(time)\n",
    "            \n",
    "            # Find the news link\n",
    "            link = article.find('a')['href']\n",
    "            news_links.append(link)\n",
    "\n",
    "        # Create a DataFrame from the extracted data\n",
    "        data = {\n",
    "            'Headline': headlines,\n",
    "            'Time': times,\n",
    "            'News Link': news_links\n",
    "        }\n",
    "        news_df = pd.DataFrame(data)\n",
    "\n",
    "        return news_df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "cnbc_url = 'https://www.cnbc.com/world/?region=world'\n",
    "cnbc_news_df = scrape_cnbc_news(cnbc_url)\n",
    "print(cnbc_news_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8bf9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_ai_articles(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the container that holds the articles\n",
    "        articles_container = soup.find('div', {'id': 'most-downloaded-articles'})\n",
    "\n",
    "        # Initialize lists to store data\n",
    "        paper_titles = []\n",
    "        authors_list = []\n",
    "        published_dates = []\n",
    "        paper_urls = []\n",
    "\n",
    "        # Loop through the articles and extract data\n",
    "        articles = articles_container.find_all('li')\n",
    "        for article in articles:\n",
    "            # Extract paper title\n",
    "            title = article.find('a', {'class': 'article-content-title'}).text.strip()\n",
    "            paper_titles.append(title)\n",
    "\n",
    "            # Extract authors (may vary based on website structure)\n",
    "            authors = article.find('div', {'class': 'authors'}).text.strip()\n",
    "            authors_list.append(authors)\n",
    "\n",
    "            # Extract published date (may vary based on website structure)\n",
    "            date = article.find('div', {'class': 'published-date'}).text.strip()\n",
    "            published_dates.append(date)\n",
    "\n",
    "            # Extract paper URL\n",
    "            url = article.find('a', {'class': 'article-content-title'})['href']\n",
    "            paper_url = f'https://www.journals.elsevier.com{url}'\n",
    "            paper_urls.append(paper_url)\n",
    "\n",
    "        # Create a DataFrame from the extracted data\n",
    "        data = {\n",
    "            'Paper Title': paper_titles,\n",
    "            'Authors': authors_list,\n",
    "            'Published Date': published_dates,\n",
    "            'Paper URL': paper_urls\n",
    "        }\n",
    "        articles_df = pd.DataFrame(data)\n",
    "\n",
    "        return articles_df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "ai_articles_url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "ai_articles_df = scrape_ai_articles(ai_articles_url)\n",
    "print(ai_articles_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26afb6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_restaurants(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the container that holds the restaurant details\n",
    "        restaurants_container = soup.find_all('div', {'class': 'restnt-card'})\n",
    "\n",
    "        # Initialize lists to store data\n",
    "        restaurant_names = []\n",
    "        cuisines_list = []\n",
    "        locations = []\n",
    "        ratings_list = []\n",
    "        image_urls = []\n",
    "\n",
    "        # Loop through the restaurant details and extract data\n",
    "        for restaurant in restaurants_container:\n",
    "            # Extract restaurant name\n",
    "            name = restaurant.find('div', {'class': 'restnt-card-index'})\n",
    "            restaurant_name = name.text.strip() if name else 'N/A'\n",
    "            restaurant_names.append(restaurant_name)\n",
    "\n",
    "            # Extract cuisine\n",
    "            cuisine = restaurant.find('div', {'class': 'restnt-cuisine-text'})\n",
    "            cuisine_text = cuisine.text.strip() if cuisine else 'N/A'\n",
    "            cuisines_list.append(cuisine_text)\n",
    "\n",
    "            # Extract location\n",
    "            location = restaurant.find('div', {'class': 'restnt-loc-text'})\n",
    "            location_text = location.text.strip() if location else 'N/A'\n",
    "            locations.append(location_text)\n",
    "\n",
    "            # Extract ratings\n",
    "            rating = restaurant.find('span', {'class': 'restnt-rating'})\n",
    "            rating_text = rating.text.strip() if rating else 'N/A'\n",
    "            ratings_list.append(rating_text)\n",
    "\n",
    "            # Extract image URL\n",
    "            image = restaurant.find('div', {'class': 'restnt-img'})\n",
    "            image_url = image.find('img')['src'] if image else 'N/A'\n",
    "            image_urls.append(image_url)\n",
    "\n",
    "        # Create a DataFrame from the extracted data\n",
    "        data = {\n",
    "            'Restaurant Name': restaurant_names,\n",
    "            'Cuisine': cuisines_list,\n",
    "            'Location': locations,\n",
    "            'Ratings': ratings_list,\n",
    "            'Image URL': image_urls\n",
    "        }\n",
    "        restaurants_df = pd.DataFrame(data)\n",
    "\n",
    "        return restaurants_df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "dineout_url = 'https://www.dineout.co.in/bangalore-restaurants'\n",
    "dineout_restaurants_df = scrape_dineout_restaurants(dineout_url)\n",
    "print(dineout_restaurants_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
